import pandas as pd
import numpy as np
from scipy import stats
from sklearn.preprocessing import StandardScaler
import warnings
from scipy.stats import ConstantInputWarning
import os
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm

# Set up matplotlib backend
try:
        import matplotlib
        matplotlib.use('Agg')
except Exception as e:
        print(f"Warning: Visualization setup error: {e}")

# Suppress specific warnings
warnings.filterwarnings('ignore', category=ConstantInputWarning)

    # Create output directories
def create_output_directories():
        """Create necessary output directories"""
        directories = ['processed_data', 'plots', 'plots/advanced']
        for directory in directories:
            os.makedirs(directory, exist_ok=True)

# Create directories at startup
create_output_directories()

def standardize_column_names(df, year):
        """
        Standardize column names based on the year of the dataset
        """
        # Print original columns for debugging
        print(f"\nOriginal columns for {year}:")
        print(df.columns.tolist())
        
        # Create a mapping dictionary for each year's columns
        if year == 2023:
            column_mapping = {
                'estimated prevalence of modern slavery per 1,000 population': 'prevalence_per_1000',
                'estimated number of people in modern slavery': 'total_victims',
                'governance issues': 'governance_score',
                'lack of basic needs': 'basic_needs_score',
                'inequality': 'inequality_score',
                'disenfranchised groups': 'disenfranchised_score',
                'effects of conflict': 'conflict_score',
                'total vulnerability score (%)': 'vulnerability_total',
                'survivors of slavery are identified and supported to exit and remain out of modern slavery (%)': 'victim_support_score',
                'criminal justice mechanisms function effectively to prevent modern slavery (%)': 'justice_score',
                'coordination occurs at the national and regional level and across borders, and governments are held to account for their response (%)': 'coordination_score',
                'risk factors, such as attitudes, social systems, and institutions that enable modern slavery are addressed (%)': 'risk_address_score',
                'government and business stop sourcing goods and services produced by forced labour (%)': 'supply_chain_score',
                'government response total (%)': 'government_response_total',
                'female': 'lfpr_female',
                'male': 'lfpr_male',
                'total_lfpr': 'lfpr_total'
            }
        elif year == 2018:
            column_mapping = {
                'est. prevalence of population in modern slavery (victims per 1,000 population)': 'prevalence_per_1000',
                'est. number of people in modern slavery': 'total_victims',
                'factor one governance issues': 'governance_score',
                'factor two nourishment and access': 'basic_needs_score',
                'factor three inequality': 'inequality_score',
                'factor four disenfranchised groups': 'disenfranchised_score',
                'factor five effects of conflict': 'conflict_score',
                'final overall (normalised, weighted) vulnerability score': 'vulnerability_total',
                'support survivors': 'victim_support_score',
                'criminal justice': 'justice_score',
                'coordination': 'coordination_score',
                'address risk': 'risk_address_score',
                'supply chains': 'supply_chain_score',
                'total': 'government_response_total',
                'female': 'lfpr_female',
                'male': 'lfpr_male',
                'total_lfpr': 'lfpr_total'
            }
        elif year == 2016:
            column_mapping = {
                'estimated proportion of population in modern slavery': 'prevalence_per_1000',
                'estimated number in modern slavery': 'total_victims',
                'dimension 1: political rights and safety': 'governance_score',
                'dimension 2: financial and health protections': 'basic_needs_score',
                'dimension 3: protection for the most vulnerable': 'inequality_score',
                'dimension 4: conflict': 'conflict_score',
                'mean vulnerability': 'vulnerability_total',
                'milestone 1: victims are supported to exit slavery (%)': 'victim_support_score',
                'milestone 2: criminal justice responses (%)': 'justice_score',
                'milestone 3: coordination and accountability (%)': 'coordination_score',
                'milestone 4: addressing risk (%)': 'risk_address_score',
                'milestone 5: investigating supply chains (%)': 'supply_chain_score',
                'total score (/100)': 'government_response_total',
                'female': 'lfpr_female',
                'male': 'lfpr_male',
                'total': 'lfpr_total'
            }
        
        # Common columns across all years
        common_columns = {
            'country': 'country',
            'population': 'population',
            'region': 'region',
            'corruption': 'corruption_score',
            'democracy score': 'democracy_score',
            'gdp per capita': 'gdp_per_capita',
            'migration': 'migration_rate'
        }

        # Before standardization, check for missing columns
        missing_mappings = []
        for old_col in df.columns:
            if old_col not in column_mapping and old_col not in ['year']:
                missing_mappings.append(old_col)
                print(f"Warning: No mapping found for column: {old_col}")
        
        # Combine the year-specific mapping with common columns
        column_mapping.update(common_columns)

        # Create new DataFrame with standardized columns
        standardized_df = pd.DataFrame()
        
        # Copy over columns that exist in the mapping
        mapped_columns = []
        for old_col, new_col in column_mapping.items():
            if old_col in df.columns:
                standardized_df[new_col] = df[old_col]
                mapped_columns.append(new_col)
        
        # Ensure year column exists
        standardized_df['year'] = year
        
        # Print standardized columns for debugging
        print(f"\nStandardized columns for {year}:")
        print(standardized_df.columns.tolist())
        
        return standardized_df

def convert_data_types(df):
        """
        Convert columns to appropriate data types and handle any formatting issues
        """
        try:
            # Dictionary of columns and their desired data types
            numeric_columns = {
                'population': 'float64',
                'total_victims': 'float64',
                'prevalence_per_1000': 'float64',
                'governance_score': 'float64',
                'basic_needs_score': 'float64',
                'inequality_score': 'float64',
                'disenfranchised_score': 'float64',
                'conflict_score': 'float64',
                'vulnerability_total': 'float64',
                'victim_support_score': 'float64',
                'justice_score': 'float64',
                'coordination_score': 'float64',
                'risk_address_score': 'float64',
                'supply_chain_score': 'float64',
                'government_response_total': 'float64',
                'corruption_score': 'float64',
                'democracy_score': 'float64',
                'gdp_per_capita': 'float64',
                'migration_rate': 'float64',
                'lfpr_female': 'float64',
                'lfpr_male': 'float64',
                'lfpr_total': 'float64'
            }
            
            for col, dtype in numeric_columns.items():
                if col in df.columns:
                    try:
                        # Remove any commas and convert to numeric
                        df[col] = df[col].astype(str).str.replace(',', '')
                        df[col] = pd.to_numeric(df[col], errors='coerce')
                        print(f"Successfully converted {col} to numeric")
                    except Exception as e:
                        print(f"Warning: Could not convert {col} to {dtype}: {str(e)}")
            
            return df
        
        except Exception as e:
            print(f"Error in data type conversion: {str(e)}")
            return df
        
def check_required_columns(df, year):
        """
        Check if all required columns are present in the dataset and have valid data
        """
        required_columns = {
            'country': str,
            'population': np.number,
            'prevalence_per_1000': np.number,
            'total_victims': np.number,
            'vulnerability_total': np.number,
            'governance_score': np.number,
            'basic_needs_score': np.number,
            'inequality_score': np.number,
            'conflict_score': np.number,
            'victim_support_score': np.number,
            'justice_score': np.number,
            'coordination_score': np.number,
            'risk_address_score': np.number,
            'supply_chain_score': np.number,
            'government_response_total': np.number
        }
        
        missing_columns = []
        invalid_data = []
        
        for col, dtype in required_columns.items():
            if col not in df.columns:
                missing_columns.append(col)
            else:
                # Check if numeric columns have valid data
                if dtype == np.number:
                    if df[col].isnull().all():
                        invalid_data.append(f"{col} (all null)")
                    elif df[col].dtype not in [np.float64, np.int64]:
                        invalid_data.append(f"{col} (wrong type: {df[col].dtype})")
        
        if missing_columns:
            print(f"\nWarning: Missing required columns for {year}:")
            for col in missing_columns:
                print(f"- {col}")
        
        if invalid_data:
            print(f"\nWarning: Invalid data in columns for {year}:")
            for issue in invalid_data:
                print(f"- {issue}")
        
        # Print summary statistics for numeric columns
        numeric_cols = [col for col in df.columns if df[col].dtype in [np.float64, np.int64]]
        if numeric_cols:
            print(f"\nSummary statistics for {year}:")
            print(df[numeric_cols].describe())
        
        return len(missing_columns) == 0 and len(invalid_data) == 0

def standardize_numeric_values(df):
        """
        Standardize numeric columns using z-score standardization
        """
        # Initialize scaler
        scaler = StandardScaler()
        
        # Identify numeric columns (excluding specific columns)
        exclude_cols = ['year', 'population']
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        numeric_cols = [col for col in numeric_cols if col not in exclude_cols]
        
        # Create copy of dataframe
        std_df = df.copy()
        
        # Standardize each numeric column
        for col in numeric_cols:
            # Handle missing values
            if std_df[col].isnull().any():
                std_df[col] = std_df[col].fillna(std_df[col].median())
            
            # Reshape for StandardScaler
            values = std_df[col].values.reshape(-1, 1)
            
            # Standardize
            std_values = scaler.fit_transform(values)
            
            # Replace values in dataframe
            std_df[col] = std_values
        
        return std_df

def process_dataset(df, year):
    """
    Process a single dataset through the standardization pipeline with validation
    """
    try:
        # Step 1: Standardize column names
        std_df = standardize_column_names(df, year)

        # Step 2: Convert data types
        std_df = convert_data_types(std_df)

        # Step 3: Validate required columns before proceeding
        if not check_required_columns(std_df, year):
            print(f"Warning: Data validation failed for {year}")
            return None  # Early exit if validation fails

        # Step 4: Handle missing values using median imputation
        numeric_cols = std_df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if std_df[col].isnull().any():
                median_val = std_df[col].median()
                std_df[col] = std_df[col].fillna(median_val)
                print(f"Filled missing values in {col} with median")

        # Step 5: Standardize numeric values using z-score normalization
        std_df = standardize_numeric_values(std_df)

        return std_df

    except Exception as e:
        print(f"Error processing dataset for year {year}: {str(e)}")
        return None

    
def perform_multiple_testing_correction(p_values, method='bonferroni', alpha=0.05):
        """
        Perform multiple testing correction on p-values
        
        Parameters:
        - p_values: List or array of p-values
        - method: Correction method ('bonferroni', 'holm', 'simes', 'fdr_bh')
        - alpha: Significance level
        
        Returns:
        - Corrected p-values
        """
        from statsmodels.stats.multitest import multipletests
        
        # Perform multiple testing correction
        reject, corrected_p, _, _ = multipletests(
            p_values, 
            alpha=alpha, 
            method=method
        )
        
        return corrected_p

def calculate_correlations(df, target_col='prevalence_per_1000'):
        """
        Calculate correlations with target column and their p-values
        """
        correlations = {}
        p_values = {}
        
        # Get numeric columns (excluding specific columns)
        exclude_cols = ['year', 'population', 'total_victims']
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        numeric_cols = [col for col in numeric_cols if col not in exclude_cols and col != target_col]
        
        for col in numeric_cols:
            try:
                # Get complete cases for both columns
                valid_data = df[[col, target_col]].dropna()
                
                if len(valid_data) > 1:  # Need at least 2 points for correlation
                    x = valid_data[col]
                    y = valid_data[target_col]
                    
                    # Check if either column is constant
                    if x.std() != 0 and y.std() != 0:
                        corr, p_val = stats.pearsonr(x, y)
                        correlations[col] = corr
                        p_values[col] = p_val
            except Exception as e:
                print(f"Warning: Could not calculate correlation for {col}: {str(e)}")
                continue
        
        return correlations, p_values
        
def validate_standardized_datasets(datasets):
        """
        Validate standardized datasets across years
        """
        print("\nValidating standardized datasets...")
        
        # Check for consistent columns across years
        all_columns = set()
        for year, df in datasets.items():
            all_columns.update(df.columns)
        
        print("\nChecking column consistency across years:")
        for col in sorted(all_columns):
            years_present = [year for year, df in datasets.items() if col in df.columns]
            if len(years_present) != len(datasets):
                print(f"Warning: Column '{col}' only present in years: {years_present}")
        
        # Check value ranges for key metrics
        key_metrics = ['prevalence_per_1000', 'vulnerability_total', 'government_response_total']
        print("\nChecking value ranges for key metrics:")
        for metric in key_metrics:
            print(f"\n{metric}:")
            for year, df in datasets.items():
                if metric in df.columns:
                    print(f"{year}:")
                    print(df[metric].describe())
        
        return True

def ensure_consistent_columns(datasets):
        """
        Ensure all datasets have the same columns
        """
        try:
            print("\nEnsuring consistent columns across datasets...")
            
            # Get all unique columns
            all_columns = set()
            for df in datasets.values():
                all_columns.update(df.columns)
            
            # Core columns that should always be present
            core_columns = [
                'country', 'year', 'prevalence_per_1000', 'total_victims',
                'vulnerability_total', 'governance_score', 'basic_needs_score',
                'inequality_score', 'conflict_score', 'government_response_total',
                'victim_support_score', 'justice_score', 'coordination_score',
                'risk_address_score', 'supply_chain_score', 'corruption_score',
                'democracy_score'
            ]
            
            # Add missing columns with NaN values
            for year, df in datasets.items():
                print(f"\nProcessing {year} dataset:")
                
                # Check core columns
                missing_core = [col for col in core_columns if col not in df.columns]
                if missing_core:
                    print(f"Warning: Missing core columns: {missing_core}")
                    for col in missing_core:
                        datasets[year][col] = np.nan
                
                # Add other columns that exist in any dataset
                missing_cols = [col for col in all_columns if col not in df.columns]
                if missing_cols:
                    print(f"Adding missing columns: {missing_cols}")
                    for col in missing_cols:
                        datasets[year][col] = np.nan
                
                # Ensure consistent column order
                datasets[year] = datasets[year].reindex(columns=sorted(all_columns))
            
            return datasets
        
        except Exception as e:
            print(f"Error ensuring consistent columns: {str(e)}")
            return datasets

def create_visualizations(datasets):
        """
        Create visualizations for key relationships
        """
        try:
            import matplotlib.pyplot as plt
            import seaborn as sns
            
            # Create plots directory if it doesn't exist
            os.makedirs('plots', exist_ok=True)
            
            # Set style
            plt.style.use('default')
            sns.set_theme(style="whitegrid")
            
            # Key variables to plot
            variables = [
                'vulnerability_total', 'governance_score', 'corruption_score',
                'democracy_score', 'gdp_per_capita', 'government_response_total'
            ]
            
            # Create scatter plots
            for var in variables:
                fig = plt.figure(figsize=(15, 5))
                
                for i, (year, df) in enumerate(datasets.items(), 1):
                    if var in df.columns and 'prevalence_per_1000' in df.columns:
                        ax = fig.add_subplot(1, 3, i)
                        
                        # Create scatter plot
                        sns.scatterplot(data=df, x=var, y='prevalence_per_1000', ax=ax)
                        
                        # Add trend line
                        sns.regplot(data=df, x=var, y='prevalence_per_1000', 
                                scatter=False, color='red', ax=ax)
                        
                        # Calculate correlation
                        mask = df[[var, 'prevalence_per_1000']].notna().all(axis=1)
                        if mask.sum() > 1:
                            corr = stats.pearsonr(
                                df.loc[mask, var],
                                df.loc[mask, 'prevalence_per_1000']
                            )[0]
                            ax.set_title(f'{year}\nr = {corr:.3f}')
                        
                        ax.set_xlabel(var.replace('_', ' ').title())
                        ax.set_ylabel('Modern Slavery Prevalence')
                
                plt.tight_layout()
                plt.savefig(f'plots/{var}_relationship.png')
                plt.close()
            
            # Create correlation heatmaps
            for year, df in datasets.items():
                plt.figure(figsize=(12, 10))
                
                # Select numeric columns
                numeric_cols = df.select_dtypes(include=[np.number]).columns
                
                # Calculate correlation matrix
                corr_matrix = df[numeric_cols].corr()
                
                # Create heatmap
                sns.heatmap(corr_matrix, annot=True, cmap='RdBu_r', center=0,
                        fmt='.2f', square=True)
                
                plt.title(f'Correlation Heatmap - {year}')
                plt.tight_layout()
                plt.savefig(f'plots/correlation_heatmap_{year}.png')
                plt.close()
            
            print("\nVisualization files saved in 'plots' directory")
            
        except Exception as e:
            print(f"Error creating visualizations: {str(e)}")

def save_results(results, year):
        """
        Save standardized dataset and correlation results
        """
        try:
            # Save standardized dataset
            if 'data' in results and not results['data'].empty:
                results['data'].to_csv(f'processed_data/standardized_{year}.csv', index=False)
            
            # Create correlation results dataframe if correlations exist
            if results['correlations']:
                corr_df = pd.DataFrame({
                    'variable': list(results['correlations'].keys()),
                    'correlation': list(results['correlations'].values()),
                    'p_value': list(results['p_values'].values()),
                    'corrected_p_value': list(results['corrected_p_values'].values())
                })
                
                # Add significance indicator
                corr_df['significant'] = corr_df['corrected_p_value'] < 0.05
                
                # Sort by absolute correlation value
                corr_df['abs_correlation'] = abs(corr_df['correlation'])
                corr_df = corr_df.sort_values('abs_correlation', ascending=False)
                corr_df = corr_df.drop('abs_correlation', axis=1)
                
                # Save correlation results
                corr_df.to_csv(f'processed_data/correlations_{year}.csv', index=False)
                
                # Print summary of significant correlations
                sig_corrs = corr_df[corr_df['significant']]
                if not sig_corrs.empty:
                    print(f"\nSignificant correlations for {year}:")
                    for _, row in sig_corrs.iterrows():
                        print(f"{row['variable']}: r={row['correlation']:.3f}, p={row['corrected_p_value']:.3e}")
            
            return True
        
        except Exception as e:
            print(f"Error saving results for year {year}: {str(e)}")
            return False
        
def print_summary_statistics(df, year):
        """
        Print summary statistics for the dataset
        """
        print(f"\nSummary Statistics for {year}:")
        print("-" * 50)
        print(f"Number of observations: {len(df)}")
        print(f"Number of variables: {df.shape[1]}")
        print("\nNumeric variables summary:")
        print(df.select_dtypes(include=[np.number]).describe())

def perform_regression_analysis(datasets):
        """
        Perform regression analysis for each year
        """
        try:
            print("\nPerforming Regression Analysis:")
            
            # Independent variables
            predictors = [
                'lfpr_total', 'vulnerability_total', 'governance_score', 'corruption_score',
                'democracy_score', 'basic_needs_score', 'inequality_score',
                'conflict_score', 'government_response_total'
            ]
            
            for year, df in datasets.items():
                print(f"\nYear {year}:")
                
                # Get available predictors
                available_predictors = [var for var in predictors if var in df.columns]
                
                if 'prevalence_per_1000' not in df.columns:
                    print("Warning: No prevalence column found")
                    continue
                
                # Prepare data
                model_df = df[['prevalence_per_1000'] + available_predictors].copy()
                model_df = model_df.dropna()
                
                if len(model_df) < len(available_predictors) + 2:
                    print("Not enough complete cases for regression")
                    continue
                
                # Standardize predictors
                X = model_df[available_predictors]
                y = model_df['prevalence_per_1000']
                
                # Add constant
                X = sm.add_constant(X)
                
                # Fit model
                model = sm.OLS(y, X).fit()
                
                # Print results
                print("\nRegression Results:")
                print(f"R-squared: {model.rsquared:.4f}")
                print(f"Adjusted R-squared: {model.rsquared_adj:.4f}")
                print("\nCoefficients:")
                
                # Get coefficients and p-values
                coef_df = pd.DataFrame({
                    'coef': model.params,
                    'std_err': model.bse,
                    't_value': model.tvalues,
                    'p_value': model.pvalues
                })
                
                # Apply multiple testing correction
                coef_df['corrected_p'] = perform_multiple_testing_correction(
                    coef_df['p_value']
                )
                
                # Save results
                coef_df.to_csv(f'processed_data/regression_results_{year}.csv')
                
                # Print all coefficients
                for var in coef_df.index:
                    print(f"\n{var}:")
                    print(f"  Coefficient: {coef_df.loc[var, 'coef']:.3f}")
                    print(f"  t-value: {coef_df.loc[var, 't_value']:.3f}")
                    print(f"  Original p-value: {coef_df.loc[var, 'p_value']:.3e}")
                    print(f"  Corrected p-value: {coef_df.loc[var, 'corrected_p']:.3e}")
        
        except Exception as e:
            print(f"Error in regression analysis: {str(e)}")
            import traceback
            print(traceback.format_exc())

def analyze_correlations_with_correction(datasets):
    """
    Analyze correlations with multiple testing correction across datasets
    """
    for year, df in datasets.items():
        print(f"\nYear {year}:")
        
        # Calculate correlations and p-values
        correlations, p_values = calculate_correlations(df)
        
        # Apply multiple correction methods
        corrected_results = apply_multiple_testing_corrections({
            'correlations': correlations,
            'p_values': p_values
        })

        # Print results
        print("\nCorrelation Results with Multiple Testing Corrections:")
        for var, corr in corrected_results['correlations'].items():
            print(f"\n{var}:")
            print(f"  Correlation: {corr:.3f}")
            print(f"  Original p-value: {corrected_results['p_values'][var]:.3e}")
            for method in ['bonferroni', 'holm', 'simes']:
                print(f"  {method.capitalize()} corrected p-value: {corrected_results[f'{method}_corrected_p'][var]:.3e}")

def analyze_lfpr_correlations(datasets):
        """
        Analyze correlations between LFPR and modern slavery prevalence
        """
        print("\nAnalyzing LFPR Correlations:")
        
        lfpr_variables = [
            'lfpr_total', 'lfpr_female', 'lfpr_male',
            'prevalence_per_1000'
        ]
        
        control_variables = [
            'gdp_per_capita', 'corruption_score', 'democracy_score',
            'vulnerability_total', 'government_response_total'
        ]
        
        for year, df in datasets.items():
            print(f"\nYear {year}:")
            
            # Analyze LFPR correlations
            lfpr_results = {}
            for var in lfpr_variables[:-1]:  # Exclude prevalence
                if var in df.columns:
                    mask = df[[var, 'prevalence_per_1000']].notna().all(axis=1)
                    if mask.sum() > 1:
                        x = df.loc[mask, var]
                        y = df.loc[mask, 'prevalence_per_1000']
                        corr, p_val = stats.pearsonr(x, y)
                        lfpr_results[var] = {
                            'correlation': corr,
                            'p_value': p_val
                        }
            
            # Print LFPR correlations
            print("\nLFPR Correlations with Modern Slavery Prevalence:")
            for var, results in lfpr_results.items():
                print(f"\n{var}:")
                print(f"  r = {results['correlation']:.3f}")
                print(f"  p-value = {results['p_value']:.3e}")
            
            # Analyze by gender gap
            if 'lfpr_female' in df.columns and 'lfpr_male' in df.columns:
                df['lfpr_gender_gap'] = df['lfpr_male'] - df['lfpr_female']
                mask = df[['lfpr_gender_gap', 'prevalence_per_1000']].notna().all(axis=1)
                if mask.sum() > 1:
                    corr, p_val = stats.pearsonr(
                        df.loc[mask, 'lfpr_gender_gap'],
                        df.loc[mask, 'prevalence_per_1000']
                    )
                    print("\nLFPR Gender Gap:")
                    print(f"  r = {corr:.3f}")
                    print(f"  p-value = {p_val:.3e}")
            
            # Control variable correlations
            print("\nControl Variable Correlations:")
            for var in control_variables:
                if var in df.columns:
                    mask = df[[var, 'prevalence_per_1000']].notna().all(axis=1)
                    if mask.sum() > 1:
                        corr, p_val = stats.pearsonr(
                            df.loc[mask, var],
                            df.loc[mask, 'prevalence_per_1000']
                        )
                        print(f"\n{var}:")
                        print(f"  r = {corr:.3f}")
                        print(f"  p-value = {p_val:.3e}")

def perform_lfpr_regression(datasets):
        """
        Perform regression analysis focusing on LFPR
        """
        print("\nPerforming LFPR Regression Analysis:")
        
        for year, df in datasets.items():
            print(f"\nYear {year}:")
            
            # Prepare variables
            predictors = [
                'lfpr_total', 'lfpr_gender_gap',
                'gdp_per_capita', 'corruption_score',
                'vulnerability_total', 'government_response_total'
            ]
            
            # Calculate gender gap if possible
            if 'lfpr_female' in df.columns and 'lfpr_male' in df.columns:
                df['lfpr_gender_gap'] = df['lfpr_male'] - df['lfpr_female']
            
            # Get available predictors
            available_predictors = [var for var in predictors if var in df.columns]
            
            if 'prevalence_per_1000' not in df.columns:
                print("Warning: No prevalence column found")
                continue
            
            # Prepare data
            model_df = df[['prevalence_per_1000'] + available_predictors].copy()
            model_df = model_df.dropna()
            
            if len(model_df) < len(available_predictors) + 2:
                print("Not enough complete cases for regression")
                continue
            
            # Standardize predictors
            X = model_df[available_predictors]
            y = model_df['prevalence_per_1000']
            
            # Add constant
            X = sm.add_constant(X)
            
            # Fit model
            model = sm.OLS(y, X).fit()
            
            # Print results
            print("\nRegression Results:")
            print(f"R-squared: {model.rsquared:.4f}")
            print(f"Adjusted R-squared: {model.rsquared_adj:.4f}")
            
            # Print coefficients
            print("\nCoefficients:")
            for var in model.params.index:
                print(f"\n{var}:")
                print(f"  Coefficient: {model.params[var]:.3f}")
                print(f"  t-value: {model.tvalues[var]:.3f}")
                print(f"  p-value: {model.pvalues[var]:.3e}")

def create_advanced_visualizations(datasets):
        """
        Create advanced visualizations for correlation and regression analysis
        """
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        # Create plots directory
        os.makedirs('plots/advanced', exist_ok=True)
        
        # Correlation Heatmaps
        for year, df in datasets.items():
            plt.figure(figsize=(12, 10))
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            corr_matrix = df[numeric_cols].corr()
            
            sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', 
                        center=0, fmt='.2f', square=True)
            plt.title(f'Correlation Heatmap - {year}')
            plt.tight_layout()
            plt.savefig(f'plots/advanced/correlation_heatmap_{year}.png')
            plt.close()
        
        # Scatter plots with regression lines
        key_variables = [
            'vulnerability_total', 'governance_score', 
            'corruption_score', 'democracy_score'
        ]
        
        for var in key_variables:
            plt.figure(figsize=(15, 5))
            
            for i, (year, df) in enumerate(datasets.items(), 1):
                plt.subplot(1, 3, i)
                sns.regplot(
                    x=var, 
                    y='prevalence_per_1000', 
                    data=df, 
                    scatter_kws={'alpha':0.5},
                    line_kws={'color': 'red'}
                )
                plt.title(f'{year} - {var} vs Modern Slavery')
            
            plt.tight_layout()
            plt.savefig(f'plots/advanced/{var}_regression.png')
            plt.close()

def stratified_analysis(datasets):
        """
        Perform stratified analysis by region or income group
        """
        for year, df in datasets.items():
            # Group by region
            if 'region' in df.columns:
                regional_analysis = df.groupby('region')['prevalence_per_1000'].agg(['mean', 'median', 'std'])
                print(f"\nRegional Analysis for {year}:")
                print(regional_analysis)
            
            # Optional: Categorize by GDP or other metrics
            df['income_group'] = pd.qcut(df['gdp_per_capita'], q=4, labels=['Low', 'Medium-Low', 'Medium-High', 'High'])
            income_analysis = df.groupby('income_group')['prevalence_per_1000'].agg(['mean', 'median', 'std'])
            print(f"\nIncome Group Analysis for {year}:")
            print(income_analysis)

def robustness_checks(datasets):
        """
        Perform various robustness checks
        """
        from sklearn.linear_model import HuberRegressor
        from statsmodels.stats.outliers_influence import variance_inflation_factor
        
        def calculate_vif(X):
            """Calculate Variance Inflation Factor"""
            vif_data = pd.DataFrame()
            vif_data["Variable"] = X.columns
            vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
            return vif_data
        
        for year, df in datasets.items():
            print(f"\nRobustness Checks for {year}:")
            
            # Select predictors
            predictors = ['vulnerability_total', 'governance_score', 'corruption_score']
            X = df[predictors]
            y = df['prevalence_per_1000']
            
            # Multicollinearity Check
            print("\nVariance Inflation Factors:")
            print(calculate_vif(X))
            
            # Robust Regression
            robust_reg = HuberRegressor()
            robust_reg.fit(X, y)
            
            print("\nRobust Regression Coefficients:")
            for name, coef in zip(predictors, robust_reg.coef_):
                print(f"{name}: {coef}")


def apply_multiple_testing_corrections(correlations_dict):
        """
        Apply multiple testing corrections to correlation p-values
        """
        from statsmodels.stats.multitest import multipletests
        
        # Extract p-values
        p_values = list(correlations_dict['p_values'].values())
        
        # Apply different correction methods
        corrections = {
            'bonferroni': multipletests(p_values, method='bonferroni')[1],
            'holm': multipletests(p_values, method='holm')[1],
            'fdr_bh': multipletests(p_values, method='fdr_bh')[1]
        }
        
        # Add corrected p-values to results
        results = correlations_dict.copy()
        for method, corrected_p in corrections.items():
            results[f'{method}_corrected_p'] = dict(zip(
                correlations_dict['p_values'].keys(),
                corrected_p
            ))
        
        return results

def create_correlation_plots(datasets):
        """
        Create correlation plots using matplotlib
        """
        import matplotlib.pyplot as plt
        import seaborn as sns
        
        # Create output directory
        os.makedirs('plots', exist_ok=True)
        
        # Key variables to plot
        variables = ['lfpr_total', 'lfpr_female', 'lfpr_male', 'gdp_per_capita',
                    'corruption_score', 'vulnerability_total']
        
        for var in variables:
            plt.figure(figsize=(15, 5))
            
            for i, (year, df) in enumerate(datasets.items(), 1):
                if var in df.columns:
                    plt.subplot(1, 3, i)
                    sns.scatterplot(data=df, x=var, y='prevalence_per_1000')
                    sns.regplot(data=df, x=var, y='prevalence_per_1000', 
                            scatter=False, color='red')
                    plt.title(f'{year}')
                    plt.xlabel(var.replace('_', ' ').title())
                    plt.ylabel('Modern Slavery Prevalence')
            
            plt.tight_layout()
            plt.savefig(f'plots/{var}_relationship.png')
            plt.close()

def perform_stratified_analysis(datasets):
        """
        Perform stratified analysis by region and income level
        """
        results = {}
        
        for year, df in datasets.items():
            results[year] = {}
            
            # Regional analysis
            if 'region' in df.columns:
                regional_stats = df.groupby('region')['prevalence_per_1000'].agg([
                    'mean', 'median', 'std', 'count'
                ])
                results[year]['regional'] = regional_stats
            
            # Income level analysis (using GDP quartiles)
            if 'gdp_per_capita' in df.columns:
                df['income_group'] = pd.qcut(df['gdp_per_capita'], 
                                        q=4, 
                                        labels=['Low', 'Lower-Mid', 
                                                'Upper-Mid', 'High'])
                income_stats = df.groupby('income_group')['prevalence_per_1000'].agg([
                    'mean', 'median', 'std', 'count'
                ])
                results[year]['income'] = income_stats
        
        return results

def apply_multiple_testing_corrections(correlations_dict):
    """Apply multiple testing corrections"""
    from statsmodels.stats.multitest import multipletests
    
    # Extract p-values
    p_values = list(correlations_dict['p_values'].values())
    
    # Apply different correction methods
    corrections = {
        'bonferroni': multipletests(p_values, method='bonferroni')[1],
        'holm': multipletests(p_values, method='holm')[1],
        'simes': multipletests(p_values, method='simes')[1]
    }
    
    # Add corrected p-values to results
    results = correlations_dict.copy()
    for method, corrected_p in corrections.items():
        results[f'{method}_corrected_p'] = dict(zip(
            correlations_dict['p_values'].keys(),
            corrected_p
        ))
    
    return results

def create_correlation_plots(datasets):
    """Create correlation plots"""
    for year, df in datasets.items():
        # Create basic correlation plots
        create_visualizations({year: df})  # Using your existing create_visualizations function

def perform_stratified_analysis(datasets):
    """Perform stratified analysis"""
    results = {}
    for year, df in datasets.items():
        # Use your existing stratified_analysis function
        results[year] = stratified_analysis({year: df})
    return results

def main_standardize():
    """Main standardization function"""
    try:
        standardized_datasets = {}
        
        for year in [2016, 2018, 2023]:
            print(f"\nProcessing {year} dataset...")
            df = pd.read_csv(f'processed_data/filtered_merged_{year}.csv')
            std_df = process_dataset(df, year)
            
            if std_df is not None:
                standardized_datasets[year] = std_df
        
        return standardized_datasets
    except Exception as e:
        print(f"Error in standardization: {e}")
        return None

def analyze_correlations_with_correction(datasets):
    """Analyze correlations with multiple testing corrections"""
    results = {}
    
    for year, df in datasets.items():
        # Calculate correlations
        correlations, p_values = calculate_correlations(df)
        
        # Apply corrections
        corrected_results = apply_multiple_testing_corrections({
            'correlations': correlations,
            'p_values': p_values
        })
        
        results[year] = corrected_results
        
        # Print results
        print(f"\nResults for {year}:")
        for var in correlations.keys():
            print(f"\n{var}:")
            print(f"Correlation: {correlations[var]:.3f}")
            print(f"Original p-value: {p_values[var]:.3e}")
            for method in ['bonferroni', 'holm', 'simes']:
                print(f"{method} corrected p-value: {corrected_results[f'{method}_corrected_p'][var]:.3e}")
    
    return results

def create_advanced_visualizations(datasets):
    """Create advanced visualizations"""
    # Create plots directory
    os.makedirs('plots/advanced', exist_ok=True)
    
    for year, df in datasets.items():
        # Create correlation heatmap
        plt.figure(figsize=(12, 10))
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        corr_matrix = df[numeric_cols].corr()
        
        sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', 
                    center=0, fmt='.2f', square=True)
        plt.title(f'Correlation Heatmap - {year}')
        plt.tight_layout()
        plt.savefig(f'plots/advanced/correlation_heatmap_{year}.png')
        plt.close()

def robustness_checks(datasets):
    """Perform robustness checks"""
    from sklearn.linear_model import HuberRegressor
    
    for year, df in datasets.items():
        print(f"\nRobustness Checks for {year}:")
        
        # Basic checks
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        
        # Check for outliers
        for col in numeric_cols:
            if col != 'year':
                Q1 = df[col].quantile(0.25)
                Q3 = df[col].quantile(0.75)
                IQR = Q3 - Q1
                outliers = df[(df[col] < (Q1 - 1.5 * IQR)) | (df[col] > (Q3 + 1.5 * IQR))]
                if not outliers.empty:
                    print(f"\nOutliers in {col}: {len(outliers)}")
        
        # Robust regression
        if 'prevalence_per_1000' in df.columns:
            predictors = ['vulnerability_total', 'governance_score', 'corruption_score']
            available_predictors = [p for p in predictors if p in df.columns]
            
            if available_predictors:
                X = df[available_predictors]
                y = df['prevalence_per_1000']
                
                robust_reg = HuberRegressor()
                robust_reg.fit(X, y)
                
                print("\nRobust Regression Coefficients:")
                for name, coef in zip(available_predictors, robust_reg.coef_):
                    print(f"{name}: {coef:.3f}")

def apply_multiple_testing_corrections(correlations_dict):
    from statsmodels.stats.multitest import multipletests

    # Extract p-values correctly
    p_values = list(correlations_dict['p_values'].values())

    # Apply multiple testing corrections
    corrections = {
        'bonferroni': multipletests(p_values, method='bonferroni')[1],
        'holm': multipletests(p_values, method='holm')[1],
        'simes': multipletests(p_values, method='simes')[1]
    }

    # Fix: Proper handling of corrected p-values
    results = correlations_dict.copy()
    for method, corrected_p_values in corrections.items():
        corrected_p_dict = dict(zip(correlations_dict['p_values'].keys(), corrected_p_values))
        results[f'{method}_corrected_p'] = corrected_p_dict

    return results

def create_correlation_plots(datasets):
    """
    Create correlation plots using matplotlib
    """
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    # Create output directory
    os.makedirs('plots', exist_ok=True)
    
    # Key variables to plot
    variables = ['lfpr_total', 'lfpr_female', 'lfpr_male', 'gdp_per_capita',
                'corruption_score', 'vulnerability_total']
    
    for var in variables:
        plt.figure(figsize=(15, 5))
        
        for i, (year, df) in enumerate(datasets.items(), 1):
            if var in df.columns:
                plt.subplot(1, 3, i)
                sns.scatterplot(data=df, x=var, y='prevalence_per_1000')
                sns.regplot(data=df, x=var, y='prevalence_per_1000', 
                        scatter=False, color='red')
                plt.title(f'{year}')
                plt.xlabel(var.replace('_', ' ').title())
                plt.ylabel('Modern Slavery Prevalence')
        
        plt.tight_layout()
        plt.savefig(f'plots/{var}_relationship.png')
        plt.close()

def perform_stratified_analysis(datasets):
    """
    Perform stratified analysis by region and income level
    """
    results = {}
    
    for year, df in datasets.items():
        results[year] = {}
        
        # Regional analysis
        if 'region' in df.columns:
            regional_stats = df.groupby('region')['prevalence_per_1000'].agg([
                'mean', 'median', 'std', 'count'
            ])
            results[year]['regional'] = regional_stats
            
            print(f"\nRegional Analysis for {year}:")
            print(regional_stats)
        
        # Income level analysis (using GDP quartiles)
        if 'gdp_per_capita' in df.columns:
            df['income_group'] = pd.qcut(df['gdp_per_capita'], 
                                    q=4, 
                                    labels=['Low', 'Lower-Mid', 
                                            'Upper-Mid', 'High'])
            income_stats = df.groupby('income_group')['prevalence_per_1000'].agg([
                'mean', 'median', 'std', 'count'
            ])
            results[year]['income'] = income_stats
            
            print(f"\nIncome Group Analysis for {year}:")
            print(income_stats)
    
    return results

def main_standardize():
        """Main standardization function"""
        try:
            standardized_datasets = {}
            
            for year in [2016, 2018, 2023]:
                print(f"\nProcessing {year} dataset...")
                df = pd.read_csv(f'processed_data/filtered_merged_{year}.csv')
                std_df = process_dataset(df, year)
                
                if std_df is not None:
                    standardized_datasets[year] = std_df
            
            # Ensure consistent columns
            standardized_datasets = ensure_consistent_columns(standardized_datasets)
            
            # Perform LFPR-specific analyses
            analyze_lfpr_correlations(standardized_datasets)
            perform_lfpr_regression(standardized_datasets)
            
            # Validate
            validate_standardized_datasets(standardized_datasets)
            
            # Add these lines
            create_advanced_visualizations(standardized_datasets)
            stratified_analysis(standardized_datasets)
            robustness_checks(standardized_datasets)
            
            return standardized_datasets
        except Exception as e:
            print(f"Error in main standardization process: {str(e)}")
            return None
        
import pandas as pd
import numpy as np
import os
from scipy import stats
from sklearn.preprocessing import StandardScaler
import warnings
from scipy.stats import ConstantInputWarning

# Set up matplotlib backend
try:
    import matplotlib
    matplotlib.use('Agg')
except Exception as e:
    print(f"Warning: Visualization setup error: {e}")

# Now import matplotlib and seaborn
import matplotlib.pyplot as plt
import seaborn as sns
import statsmodels.api as sm

# Suppress specific warnings
warnings.filterwarnings('ignore', category=ConstantInputWarning)

# Create output directories
def create_output_directories():
    """Create necessary output directories"""
    directories = ['processed_data', 'plots', 'plots/advanced']
    for directory in directories:
        os.makedirs(directory, exist_ok=True)

# Create directories at startup
create_output_directories()

# Rest of your code follows...

def normalize_column_names(df):
    df.columns = df.columns.str.strip().str.lower()
    return df

def load_initial_datasets():
    try:
        # List of encodings to try
        encodings = ['utf-8', 'latin1', 'iso-8859-1', 'cp1252']
        datasets = {}
        
        files = {
            'corruption': 'datasets/corruption.csv',
            'democracy': 'datasets/democracy.csv',
            'gdppercapita': 'datasets/gdppercapita.csv',
            'lfpr': 'datasets/lfpr.csv',
            'migration': 'datasets/migration.csv',
            'slavery_2023': 'datasets/slavery_2023.csv',
            'slavery_2018': 'datasets/slavery_2018.csv',
            'slavery_2016': 'datasets/slavery_2016.csv'
        }
        
        for name, file_path in files.items():
            success = False
            for encoding in encodings:
                try:
                    print(f"Trying to load {name} with {encoding} encoding...")
                    df = pd.read_csv(file_path, encoding=encoding)
                    datasets[name] = df
                    print(f"Successfully loaded {name} with {encoding} encoding")
                    success = True
                    break
                except UnicodeDecodeError:
                    continue
                except FileNotFoundError:
                    print(f"File not found: {file_path}")
                    raise
            
            if not success:
                raise ValueError(f"Failed to load {name} with any encoding")
        
        # Normalize column names for all datasets
        return {name: normalize_column_names(df) for name, df in datasets.items()}
    
    except Exception as e:
        print(f"Error loading datasets: {e}")
        raise

def verify_datasets(dfs):
    for name, df in dfs.items():
        print(f"\nVerifying dataset: {name}")
        print(f"Shape: {df.shape}")
        print(f"Columns: {df.columns.tolist()}")
        
        # Check for required columns
        required_cols = ['country', 'year']
        missing_cols = [col for col in required_cols if col not in df.columns]
        
        if missing_cols:
            print(f"Warning: Missing required columns: {missing_cols}")
        
        # Check for null values
        null_counts = df.isnull().sum()
        if null_counts.any():
            print("Null value counts:")
            print(null_counts[null_counts > 0])

def standardize_country_names(df):
    country_mapping = {
        "United States": "USA",
        "United Kingdom": "UK",
        "United States of America": "USA",
        "Great Britain": "UK",
        # Add more mappings as needed
    }
    df['country'] = df['country'].replace(country_mapping)
    return df

def preprocess_lfpr(df):
    try:
        # Ensure required columns exist
        required_cols = ['country', 'year', 'type_lfpr', 'lfpr']
        if not all(col in df.columns for col in required_cols):
            raise ValueError(f"LFPR dataset missing required columns. Found: {df.columns.tolist()}")
        
        # Pivot LFPR data
        lfpr_pivoted = df.pivot_table(
            index=['country', 'year'],
            columns='type_lfpr',
            values='lfpr'
        ).reset_index()
        
        # Rename columns to lowercase
        lfpr_pivoted.columns = lfpr_pivoted.columns.str.lower()
        
        return lfpr_pivoted
    
    except Exception as e:
        print(f"Error preprocessing LFPR data: {e}")
        raise

def merge_datasets(dfs, target_year):
    try:
        slavery_key = f'slavery_{target_year}'
        if slavery_key not in dfs:
            raise KeyError(f"Missing slavery dataset for year {target_year}")
        
        # Start with standardized slavery dataset
        merged_df = standardize_country_names(dfs[slavery_key].copy())
        
        # Merge with other datasets
        for name, df in dfs.items():
            if name not in ['slavery_2023', 'slavery_2018', 'slavery_2016']:
                print(f"Processing {name} dataset...")
                
                # Standardize country names
                df = standardize_country_names(df)
                
                # Preprocess LFPR if needed
                if name == 'lfpr':
                    df = preprocess_lfpr(df)
                
                # Filter for target year and merge
                year_data = df[df['year'] == target_year]
                if not year_data.empty:
                    merged_df = merged_df.merge(
                        year_data,
                        on='country',
                        how='left',
                        suffixes=('', f'_{name}')
                    )
                    print(f"Merged {name} data: {merged_df.shape}")
                else:
                    print(f"Warning: No data for {name} in year {target_year}")
        
        return merged_df
    
    except Exception as e:
        print(f"Error merging datasets: {e}")
        raise


def main_process():
    try:
        # Create output directory if it doesn't exist
        import os
        os.makedirs('processed_data', exist_ok=True)
        
        # Load and verify datasets
        print("Loading datasets...")
        datasets = load_initial_datasets()
        
        print("\nVerifying datasets...")
        verify_datasets(datasets)
        
        # Process each year
        for year in [2023, 2018, 2016]:
            print(f"\nProcessing year {year}...")
            merged_df = merge_datasets(datasets, year)
            
            # Save merged dataset
            output_path = f'processed_data/merged_{year}.csv'
            merged_df.to_csv(output_path, index=False)
            print(f"Saved merged dataset to {output_path}")
            
            # Print summary statistics
            print(f"Final shape: {merged_df.shape}")
            print("Columns:", merged_df.columns.tolist())
    
    except Exception as e:
        print(f"Error in main execution: {e}")
        raise

def load_merged_datasets():
    """Load all merged datasets and return basic information about them"""
    years = [2016, 2018, 2023]
    datasets = {}
    
    for year in years:
        try:
            df = pd.read_csv(f'processed_data/merged_{year}.csv')
            datasets[year] = df
            
            print(f"\nMerged Dataset {year}:")
            print("-" * 50)
            print(f"Shape: {df.shape}")
            print("\nFirst few rows:")
            print(df.head())
            print("\nColumns:")
            print(df.columns.tolist())
            print("\nMissing values:")
            print(df.isnull().sum()[df.isnull().sum() > 0])
            print("\nSummary statistics:")
            print(df.describe())
            
            # Check which countries are included
            print(f"\nNumber of countries: {df['country'].nunique()}")
            print("\nSample of countries:")
            print(df['country'].sample(min(5, len(df['country']))).tolist())
            
        except FileNotFoundError:
            print(f"Warning: Could not find merged_{year}.csv in processed_data directory")
        except Exception as e:
            print(f"Error processing {year} dataset: {e}")
    
    return datasets

def compare_datasets(datasets):
    """Compare the different year datasets"""
    try:
        if not datasets:
            print("No datasets available for comparison")
            return
            
        years = list(datasets.keys())
        
        print("\nDataset Comparison:")
        print("-" * 50)
        
        # Compare number of countries
        print("\nNumber of countries in each dataset:")
        for year in years:
            print(f"{year}: {datasets[year]['country'].nunique()} countries")
        
        # Find common countries
        common_countries = set(datasets[years[0]]['country'])
        for year in years[1:]:
            common_countries = common_countries.intersection(set(datasets[year]['country']))
        
        print(f"\nNumber of common countries across all years: {len(common_countries)}")
        print("\nSample of common countries:")
        print(list(common_countries)[:5])
        
    except Exception as e:
        print(f"Error comparing datasets: {e}")

def load_merged_datasets():
    """Load all merged datasets"""
    try:
        datasets = {}
        for year in [2016, 2018, 2023]:
            df = pd.read_csv(f'processed_data/merged_{year}.csv')
            datasets[year] = df
        return datasets
    except Exception as e:
        print(f"Error loading datasets: {e}")
        return None

def get_common_countries(datasets):
    """Get list of countries common to all datasets"""
    try:
        # Get sets of countries from each dataset
        country_sets = [set(df['country'].unique()) for df in datasets.values()]
        
        # Get intersection of all sets
        common_countries = set.intersection(*country_sets)
        
        print(f"Number of common countries: {len(common_countries)}")
        print("\nSample of common countries:")
        print(sorted(list(common_countries))[:10])
        
        return sorted(list(common_countries))
    except Exception as e:
        print(f"Error getting common countries: {e}")
        return None

def filter_and_save_datasets(datasets, common_countries):
    """Filter datasets to include only common countries and save"""
    try:
        filtered_datasets = {}
        
        for year, df in datasets.items():
            # Filter for common countries
            filtered_df = df[df['country'].isin(common_countries)].copy()
            
            # Sort by country name for consistency
            filtered_df = filtered_df.sort_values('country').reset_index(drop=True)
            
            # Save filtered dataset
            output_path = f'processed_data/filtered_merged_{year}.csv'
            filtered_df.to_csv(output_path, index=False)
            
            filtered_datasets[year] = filtered_df
            
            print(f"\nDataset {year}:")
            print(f"Original shape: {df.shape}")
            print(f"Filtered shape: {filtered_df.shape}")
            
            # Verify all countries are present
            missing_countries = set(common_countries) - set(filtered_df['country'])
            if missing_countries:
                print(f"Warning: Missing countries in {year}: {missing_countries}")
    
        return filtered_datasets
    except Exception as e:
        print(f"Error filtering datasets: {e}")
        return None

def verify_filtered_datasets(filtered_datasets):
    """Verify that filtered datasets contain the same countries"""
    try:
        print("\nVerifying filtered datasets:")
        
        # Check country counts
        for year, df in filtered_datasets.items():
            print(f"\n{year} dataset:")
            print(f"Number of countries: {df['country'].nunique()}")
            print("First 5 countries:")
            print(df['country'].head().tolist())
            
        # Verify columns
        print("\nColumns in each dataset:")
        for year, df in filtered_datasets.items():
            print(f"\n{year} columns:")
            print(df.columns.tolist())
            
    except Exception as e:
        print(f"Error verifying datasets: {e}")

def main_analyze():
    try:
        # Load original merged datasets
        print("Loading datasets...")
        datasets = load_merged_datasets()
        
        if datasets:
            # Get common countries
            print("\nFinding common countries...")
            common_countries = get_common_countries(datasets)
            
            if common_countries:
                # Filter and save datasets
                print("\nFiltering datasets...")
                filtered_datasets = filter_and_save_datasets(datasets, common_countries)
                
                if filtered_datasets:
                    # Verify filtered datasets
                    verify_filtered_datasets(filtered_datasets)
                    
                    print("\nSuccess! Filtered datasets have been saved to processed_data/")
    
    except Exception as e:
        print(f"Error in main execution: {e}")
        
def process_initial_data():
    """Initial data processing function - runs first"""
    try:
        # Create output directory if it doesn't exist
        os.makedirs('processed_data', exist_ok=True)
        
        # Load and verify datasets
        print("Loading datasets...")
        datasets = load_initial_datasets()
        
        print("\nVerifying datasets...")
        verify_datasets(datasets)
        
        # Process each year
        for year in [2023, 2018, 2016]:
            print(f"\nProcessing year {year}...")
            merged_df = merge_datasets(datasets, year)
            
            # Save merged dataset
            output_path = f'processed_data/merged_{year}.csv'
            merged_df.to_csv(output_path, index=False)
            print(f"Saved merged dataset to {output_path}")
            
            # Print summary statistics
            print(f"Final shape: {merged_df.shape}")
            print("Columns:", merged_df.columns.tolist())
        
        return True
    
    except Exception as e:
        print(f"Error in initial data processing: {e}")
        return False

def analyze_processed_data():
    """Analysis function - runs after initial processing"""
    try:
        print("\nLoading and analyzing merged datasets...")
        datasets = load_merged_datasets()
        if datasets:
            compare_datasets(datasets)
            
            # Get common countries and filter datasets
            common_countries = get_common_countries(datasets)
            if common_countries:
                filtered_datasets = filter_and_save_datasets(datasets, common_countries)
                if filtered_datasets:
                    verify_filtered_datasets(filtered_datasets)
    
    except Exception as e:
        print(f"Error in data analysis: {e}")

def analyze_lfpr_correlations(datasets):
    """
    Enhanced LFPR correlation analysis with multiple testing correction
    """
    for year, df in datasets.items():
        print(f"\nYear {year}:")
        
        # Calculate correlations
        correlations = calculate_correlations(df)
        
        # Apply multiple testing corrections
        corrected_results = apply_multiple_testing_corrections(correlations)
        
        # Create visualizations
        create_correlation_plots({year: df})
        
        # Perform stratified analysis
        stratified_results = perform_stratified_analysis({year: df})
        
        # Print results
        print("\nCorrelation Results with Multiple Testing Corrections:")
        for var, corr in corrected_results['correlations'].items():
            print(f"\n{var}:")
            print(f"  r = {corr:.3f}")
            print(f"  Original p-value: {corrected_results['p_values'][var]:.3e}")
            print(f"  Bonferroni p-value: {corrected_results['bonferroni_corrected_p'][var]:.3e}")
            print(f"  Holm p-value: {corrected_results['holm_corrected_p'][var]:.3e}")
            print(f"  FDR p-value: {corrected_results['fdr_bh_corrected_p'][var]:.3e}")

# Ensure the datasets are processed and stored properly
processed_datasets = main_standardize()  # Ensure this runs successfully before further analysis

if processed_datasets:
    print("\nStep 6: Creating visualizations and performing analyses...")
    create_visualizations(processed_datasets)
else:
    print("Error: No datasets were successfully processed. Check the earlier steps.")

if __name__ == "__main__":
    try:
        # Previous code remains the same until processed_datasets is created
        
        if processed_datasets:
            print("\nStep 6: Creating visualizations and performing analyses...")
            
            # Create basic visualizations
            create_visualizations(processed_datasets)
            
            # Create correlation plots
            create_correlation_plots(processed_datasets)
            
            # Perform stratified analysis
            stratified_results = perform_stratified_analysis(processed_datasets)
            
            # Apply multiple testing corrections to correlation results
            for year, df in processed_datasets.items():
                correlations, p_values = calculate_correlations(df)
                corrected_results = apply_multiple_testing_corrections({
                    'correlations': correlations,
                    'p_values': p_values
                })
                
                # Print results
                print(f"\nResults for {year} with multiple testing corrections:")
                for var in correlations.keys():
                    print(f"\n{var}:")
                    print(f"Correlation: {correlations[var]:.3f}")
                    print(f"Original p-value: {p_values[var]:.3e}")
                    for method in ['bonferroni', 'holm', 'fdr_bh']:
                        print(f"{method} corrected p-value: {corrected_results[f'{method}_corrected_p'][var]:.3e}")
            
            print("\nAnalysis complete! Check the 'processed_data' and 'plots' directories for results.")
        else:
            print("No datasets were successfully processed")
            
    except Exception as e:
        print(f"Error in main execution: {str(e)}")
        import traceback
        print(traceback.format_exc())